PROTOC src/caffe/proto/caffe.proto
CXX src/caffe/util/db.cpp
CXX src/caffe/util/im2col.cpp
CXX src/caffe/util/insert_splits.cpp
CXX src/caffe/util/blocking_queue.cpp
CXX src/caffe/util/signal_handler.cpp
CXX src/caffe/util/cudnn.cpp
CXX src/caffe/util/io.cpp
CXX src/caffe/util/benchmark.cpp
CXX src/caffe/util/db_lmdb.cpp
CXX src/caffe/util/math_functions.cpp
CXX src/caffe/util/hdf5.cpp
CXX src/caffe/util/db_leveldb.cpp
CXX src/caffe/util/upgrade_proto.cpp
CXX src/caffe/layer.cpp
CXX src/caffe/layers/exp_layer.cpp
CXX src/caffe/layers/split_layer.cpp
CXX src/caffe/layers/flatten_layer.cpp
CXX src/caffe/layers/hdf5_output_layer.cpp
CXX src/caffe/layers/cudnn_softmax_layer.cpp
CXX src/caffe/layers/hinge_loss_layer.cpp
CXX src/caffe/layers/bnll_layer.cpp
CXX src/caffe/layers/cudnn_conv_layer.cpp
CXX src/caffe/layers/softmax_loss_layer.cpp
CXX src/caffe/layers/mvn_layer.cpp
CXX src/caffe/layers/power_layer.cpp
CXX src/caffe/layers/reshape_layer.cpp
CXX src/caffe/layers/window_data_layer.cpp
CXX src/caffe/layers/reduction_layer.cpp
CXX src/caffe/layers/silence_layer.cpp
CXX src/caffe/layers/absval_layer.cpp
CXX src/caffe/layers/prelu_layer.cpp
CXX src/caffe/layers/neuron_layer.cpp
CXX src/caffe/layers/euclidean_loss_layer.cpp
CXX src/caffe/layers/conv_layer.cpp
CXX src/caffe/layers/spp_layer.cpp
CXX src/caffe/layers/multinomial_logistic_loss_layer.cpp
CXX src/caffe/layers/contrastive_loss_layer.cpp
CXX src/caffe/layers/lrn_layer.cpp
CXX src/caffe/layers/threshold_layer.cpp
CXX src/caffe/layers/loss_layer.cpp
CXX src/caffe/layers/slice_layer.cpp
CXX src/caffe/layers/inner_product_layer.cpp
CXX src/caffe/layers/argmax_layer.cpp
CXX src/caffe/layers/base_conv_layer.cpp
CXX src/caffe/layers/filter_layer.cpp
CXX src/caffe/layers/deconv_layer.cpp
CXX src/caffe/layers/accuracy_layer.cpp
CXX src/caffe/layers/image_data_layer.cpp
CXX src/caffe/layers/dummy_data_layer.cpp
CXX src/caffe/layers/cudnn_sigmoid_layer.cpp
CXX src/caffe/layers/tanh_layer.cpp
CXX src/caffe/layers/im2col_layer.cpp
CXX src/caffe/layers/tile_layer.cpp
CXX src/caffe/layers/log_layer.cpp
CXX src/caffe/layers/infogain_loss_layer.cpp
CXX src/caffe/layers/relu_layer.cpp
CXX src/caffe/layers/eltwise_layer.cpp
CXX src/caffe/layers/cudnn_relu_layer.cpp
CXX src/caffe/layers/embed_layer.cpp
CXX src/caffe/layers/dropout_layer.cpp
CXX src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp
CXX src/caffe/layers/base_data_layer.cpp
CXX src/caffe/layers/cudnn_tanh_layer.cpp
CXX src/caffe/layers/sigmoid_layer.cpp
CXX src/caffe/layers/pooling_layer.cpp
CXX src/caffe/layers/hdf5_data_layer.cpp
CXX src/caffe/layers/data_layer.cpp
CXX src/caffe/layers/memory_data_layer.cpp
CXX src/caffe/layers/softmax_layer.cpp
CXX src/caffe/layers/cudnn_pooling_layer.cpp
CXX src/caffe/layers/concat_layer.cpp
CXX src/caffe/solver.cpp
CXX src/caffe/common.cpp
CXX src/caffe/data_reader.cpp
CXX src/caffe/layer_factory.cpp
CXX src/caffe/parallel.cpp
CXX src/caffe/internal_thread.cpp
CXX src/caffe/data_transformer.cpp
CXX src/caffe/syncedmem.cpp
CXX src/caffe/net.cpp
CXX src/caffe/blob.cpp
NVCC src/caffe/util/math_functions.cu
NVCC src/caffe/util/im2col.cu
NVCC src/caffe/layers/embed_layer.cu
NVCC src/caffe/layers/dropout_layer.cu
NVCC src/caffe/layers/tile_layer.cu
NVCC src/caffe/layers/cudnn_relu_layer.cu
NVCC src/caffe/layers/filter_layer.cu
NVCC src/caffe/layers/bnll_layer.cu
NVCC src/caffe/layers/eltwise_layer.cu
NVCC src/caffe/layers/hdf5_data_layer.cu
NVCC src/caffe/layers/cudnn_pooling_layer.cu
NVCC src/caffe/layers/power_layer.cu
NVCC src/caffe/layers/base_data_layer.cu
NVCC src/caffe/layers/log_layer.cu
NVCC src/caffe/layers/concat_layer.cu
NVCC src/caffe/layers/threshold_layer.cu
NVCC src/caffe/layers/tanh_layer.cu
NVCC src/caffe/layers/split_layer.cu
NVCC src/caffe/layers/exp_layer.cu
NVCC src/caffe/layers/pooling_layer.cu
NVCC src/caffe/layers/sigmoid_layer.cu
NVCC src/caffe/layers/cudnn_conv_layer.cu
NVCC src/caffe/layers/softmax_loss_layer.cu
NVCC src/caffe/layers/absval_layer.cu
NVCC src/caffe/layers/euclidean_loss_layer.cu
NVCC src/caffe/layers/hdf5_output_layer.cu
NVCC src/caffe/layers/lrn_layer.cu
NVCC src/caffe/layers/sigmoid_cross_entropy_loss_layer.cu
NVCC src/caffe/layers/prelu_layer.cu
NVCC src/caffe/layers/inner_product_layer.cu
NVCC src/caffe/layers/softmax_layer.cu
NVCC src/caffe/layers/silence_layer.cu
NVCC src/caffe/layers/mvn_layer.cu
NVCC src/caffe/layers/cudnn_sigmoid_layer.cu
NVCC src/caffe/layers/deconv_layer.cu
NVCC src/caffe/layers/cudnn_tanh_layer.cu
NVCC src/caffe/layers/cudnn_softmax_layer.cu
NVCC src/caffe/layers/conv_layer.cu
NVCC src/caffe/layers/contrastive_loss_layer.cu
NVCC src/caffe/layers/im2col_layer.cu
NVCC src/caffe/layers/reduction_layer.cu
NVCC src/caffe/layers/relu_layer.cu
NVCC src/caffe/layers/slice_layer.cu
CXX tools/device_query.cpp
CXX tools/caffe.cpp
CXX tools/net_speed_benchmark.cpp
CXX tools/train_net.cpp
CXX tools/upgrade_net_proto_text.cpp
CXX tools/compute_image_mean.cpp
CXX tools/finetune_net.cpp
CXX tools/test_net.cpp
CXX tools/extract_features.cpp
CXX tools/convert_imageset.cpp
CXX tools/upgrade_net_proto_binary.cpp
CXX examples/mnist/convert_mnist_data.cpp
CXX examples/cpp_classification/classification.cpp
CXX examples/cifar10/convert_cifar_data.cpp
CXX examples/siamese/convert_mnist_siamese_data.cpp
CXX .build_release/src/caffe/proto/caffe.pb.cc
AR -o .build_release/lib/libcaffe.a
LD -o .build_release/lib/libcaffe.so
CXX/LD -o .build_release/tools/device_query.bin
CXX/LD -o .build_release/tools/caffe.bin
CXX/LD -o .build_release/tools/net_speed_benchmark.bin
CXX/LD -o .build_release/tools/train_net.bin
CXX/LD -o .build_release/tools/upgrade_net_proto_text.bin
CXX/LD -o .build_release/tools/compute_image_mean.bin
CXX/LD -o .build_release/tools/finetune_net.bin
CXX/LD -o .build_release/tools/extract_features.bin
CXX/LD -o .build_release/tools/test_net.bin
CXX/LD -o .build_release/tools/convert_imageset.bin
CXX/LD -o .build_release/examples/mnist/convert_mnist_data.bin
CXX/LD -o .build_release/tools/upgrade_net_proto_binary.bin
CXX/LD -o .build_release/examples/cpp_classification/classification.bin
CXX/LD -o .build_release/examples/cifar10/convert_cifar_data.bin
CXX/LD -o .build_release/examples/siamese/convert_mnist_siamese_data.bin
I0113 13:34:47.302901 22138 caffe.cpp:297] Use GPU with device ID 0
==22138== NVPROF is profiling process 22138, command: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv1.prototxt --iterations=1 --gpu 0 --logtostderr=1
I0113 13:34:50.112696 22138 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: proto_forceGradInput/conv1.prototxt
I0113 13:34:50.112812 22138 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0113 13:34:50.115181 22138 net.cpp:50] Initializing net from parameters: 
name: "ConvLayer_3x96x11x11"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 128
input_dim: 128
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
I0113 13:34:50.115254 22138 net.cpp:435] Input 0 -> data
I0113 13:34:50.133590 22138 layer_factory.hpp:76] Creating layer conv1
I0113 13:34:50.133641 22138 net.cpp:110] Creating Layer conv1
I0113 13:34:50.133658 22138 net.cpp:477] conv1 <- data
I0113 13:34:50.133680 22138 net.cpp:433] conv1 -> conv1
I0113 13:34:50.136224 22138 net.cpp:155] Setting up conv1
I0113 13:34:50.136266 22138 net.cpp:163] Top shape: 128 96 118 118 (171098112)
I0113 13:34:50.136299 22138 net.cpp:240] conv1 does not need backward computation.
I0113 13:34:50.136312 22138 net.cpp:283] This network produces output conv1
I0113 13:34:50.136328 22138 net.cpp:297] Network initialization done.
I0113 13:34:50.136338 22138 net.cpp:298] Memory required for data: 684392448
I0113 13:34:50.136366 22138 caffe.cpp:309] Performing Forward
I0113 13:34:50.149641 22138 caffe.cpp:314] Initial loss: 0
I0113 13:34:50.149683 22138 caffe.cpp:315] Performing Backward
I0113 13:34:50.238536 22138 caffe.cpp:323] *** Benchmark begins ***
I0113 13:34:50.238560 22138 caffe.cpp:324] Testing for 1 iterations.
I0113 13:34:51.304033 22138 caffe.cpp:352] Iteration: 1 forward-backward time: 579.846 ms.
I0113 13:34:51.304071 22138 caffe.cpp:355] Average time per layer: 
I0113 13:34:51.304080 22138 caffe.cpp:358]      conv1	forward: 143.14 ms.
I0113 13:34:51.304091 22138 caffe.cpp:361]      conv1	backward: 436.64 ms.
I0113 13:34:51.304114 22138 caffe.cpp:366] Average Forward pass: 143.163 ms.
I0113 13:34:51.304127 22138 caffe.cpp:368] Average Backward pass: 436.658 ms.
I0113 13:34:51.304147 22138 caffe.cpp:370] Average Forward-Backward: 579.939 ms.
I0113 13:34:51.304163 22138 caffe.cpp:372] Total Time: 579.939 ms.
I0113 13:34:51.304174 22138 caffe.cpp:373] *** Benchmark ends ***
==22138== Profiling application: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv1.prototxt --iterations=1 --gpu 0 --logtostderr=1
==22138== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 42.44%  498.79ms       256  1.9484ms  1.9269ms  1.9736ms  sgemm_sm35_ldg_tn_32x16x64x8x16
 17.93%  210.70ms       256  823.05us  810.76us  832.56us  sgemm_sm35_ldg_nn_64x16x64x16x16
  8.86%  104.10ms       512  203.32us  180.39us  223.05us  void caffe::im2col_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, caffe::im2col_gpu_kernel<float>*)
  7.99%  93.954ms       256  367.01us  344.37us  404.47us  sgemm_sm35_ldg_nt_128x16x64x16x16
  7.97%  93.714ms       256  366.07us  359.70us  375.09us  sgemm_sm_heavy_nt_ldg
  7.07%  83.059ms       256  324.45us  321.91us  326.90us  void gemv2T_kernel_val<float, int=128, int=16, int=2, int=2, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)
  4.33%  50.852ms       256  198.64us  194.67us  203.05us  void caffe::col2im_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, caffe::col2im_gpu_kernel<float>*)
  1.81%  21.219ms       256  82.887us  81.189us  84.837us  void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=0, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)
  0.99%  11.661ms       256  45.549us  41.410us  52.323us  sgemm_sm35_ldg_nt_64x16x128x8x32
  0.61%  7.1664ms         7  1.0238ms  3.6480us  3.4049ms  [CUDA memset]
  0.00%  36.866us         5  7.3730us  1.2480us  16.385us  [CUDA memcpy HtoD]

==22138== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 33.47%  554.04ms         6  92.341ms  4.1030us  420.01ms  cudaEventSynchronize
 29.59%  489.92ms         5  97.984ms  28.204us  485.64ms  cudaMemcpy
 28.77%  476.26ms        12  39.688ms  9.6740us  457.19ms  cudaFree
  6.02%  99.629ms      2560  38.917us  9.3560us  833.42us  cudaLaunch
  1.20%  19.846ms        17  1.1674ms  12.093us  18.239ms  cudaMallocHost
  0.44%  7.2736ms     31744     229ns     164ns  349.23us  cudaSetupArgument
  0.14%  2.3526ms        13  180.97us  13.113us  612.51us  cudaMalloc
  0.06%  947.77us        17  55.751us  11.954us  258.52us  cudaFreeHost
  0.06%  920.02us      2560     359ns     186ns  3.7460us  cudaConfigureCall
  0.05%  890.92us       780  1.1420us     613ns  11.941us  cudaEventRecord
  0.05%  873.83us      1024     853ns     483ns  3.8720us  cudaStreamWaitEvent
  0.04%  661.69us       249  2.6570us     144ns  114.62us  cuDeviceGetAttribute
  0.03%  440.36us      1280     344ns     271ns  2.5680us  cudaGetLastError
  0.02%  318.03us       256  1.2420us     967ns  3.9600us  cudaStreamGetPriority
  0.02%  284.79us       768     370ns     266ns  2.6130us  cudaPeekAtLastError
  0.01%  237.68us         1  237.68us  237.68us  237.68us  cudaGetDeviceProperties
  0.01%  198.81us         7  28.401us  11.971us  57.926us  cudaMemset
  0.01%  86.270us         3  28.756us  21.154us  38.531us  cuDeviceGetName
  0.01%  85.337us         3  28.445us  26.389us  31.823us  cuDeviceTotalMem
  0.00%  68.600us         2  34.300us  26.783us  41.817us  cudaStreamCreateWithFlags
  0.00%  49.881us        23  2.1680us     483ns  20.809us  cudaGetDevice
  0.00%  29.779us         7  4.2540us  2.8760us  7.0240us  cudaEventElapsedTime
  0.00%  21.294us        10  2.1290us     986ns  10.630us  cudaEventCreate
  0.00%  21.232us        18  1.1790us     752ns  3.3300us  cudaEventCreateWithFlags
  0.00%  17.470us        20     873ns     409ns  1.9000us  cudaSetDevice
  0.00%  12.133us        10  1.2130us     786ns  3.5170us  cudaEventDestroy
  0.00%  7.6420us        10     764ns     391ns  2.9640us  cudaDeviceGetAttribute
  0.00%  2.6420us         4     660ns     300ns  1.4440us  cuDeviceGetCount
  0.00%  1.9970us         2     998ns     965ns  1.0320us  cuInit
  0.00%  1.3970us         4     349ns     172ns     502ns  cuDeviceGet
  0.00%  1.3510us         2     675ns     543ns     808ns  cuDriverGetVersion
I0113 13:34:52.077365 22152 caffe.cpp:297] Use GPU with device ID 0
==22152== NVPROF is profiling process 22152, command: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv2.prototxt --iterations=1 --gpu 0 --logtostderr=1
I0113 13:34:54.882074 22152 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: proto_forceGradInput/conv2.prototxt
I0113 13:34:54.882164 22152 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0113 13:34:54.882277 22152 net.cpp:50] Initializing net from parameters: 
name: "ConvLayer_64x128x9x9"
input: "data"
input_dim: 128
input_dim: 3
input_dim: 128
input_dim: 128
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "data"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
I0113 13:34:54.882325 22152 net.cpp:435] Input 0 -> data
I0113 13:34:54.897527 22152 layer_factory.hpp:76] Creating layer conv2
I0113 13:34:54.897564 22152 net.cpp:110] Creating Layer conv2
I0113 13:34:54.897577 22152 net.cpp:477] conv2 <- data
I0113 13:34:54.897593 22152 net.cpp:433] conv2 -> conv2
I0113 13:34:54.899451 22152 net.cpp:155] Setting up conv2
I0113 13:34:54.899487 22152 net.cpp:163] Top shape: 128 96 126 126 (195084288)
I0113 13:34:54.899514 22152 net.cpp:240] conv2 does not need backward computation.
I0113 13:34:54.899524 22152 net.cpp:283] This network produces output conv2
I0113 13:34:54.899538 22152 net.cpp:297] Network initialization done.
I0113 13:34:54.899545 22152 net.cpp:298] Memory required for data: 780337152
I0113 13:34:54.899567 22152 caffe.cpp:309] Performing Forward
I0113 13:34:54.910749 22152 caffe.cpp:314] Initial loss: 0
I0113 13:34:54.910784 22152 caffe.cpp:315] Performing Backward
I0113 13:34:54.929854 22152 caffe.cpp:323] *** Benchmark begins ***
I0113 13:34:54.929873 22152 caffe.cpp:324] Testing for 1 iterations.
I0113 13:34:55.180838 22152 caffe.cpp:352] Iteration: 1 forward-backward time: 136.208 ms.
I0113 13:34:55.180891 22152 caffe.cpp:355] Average time per layer: 
I0113 13:34:55.180902 22152 caffe.cpp:358]      conv2	forward: 36.9829 ms.
I0113 13:34:55.180917 22152 caffe.cpp:361]      conv2	backward: 99.1663 ms.
I0113 13:34:55.180943 22152 caffe.cpp:366] Average Forward pass: 37.0023 ms.
I0113 13:34:55.180958 22152 caffe.cpp:368] Average Backward pass: 99.1824 ms.
I0113 13:34:55.180974 22152 caffe.cpp:370] Average Forward-Backward: 136.322 ms.
I0113 13:34:55.180990 22152 caffe.cpp:372] Total Time: 136.322 ms.
I0113 13:34:55.181001 22152 caffe.cpp:373] *** Benchmark ends ***
==22152== Profiling application: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv2.prototxt --iterations=1 --gpu 0 --logtostderr=1
==22152== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 34.85%  94.997ms       256  371.08us  367.67us  374.20us  void gemv2T_kernel_val<float, int=128, int=16, int=2, int=2, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)
 15.85%  43.220ms       256  168.83us  153.38us  177.07us  void sgemm_largek_lds64<bool=1, bool=0, int=5, int=5, int=4, int=4, int=4, int=34>(float*, float const *, float const *, int, int, int, int, int, int, float const *, float const *, float, float, int, int, int*, int*)
 15.44%  42.099ms       256  164.45us  163.21us  165.90us  sgemm_sm35_ldg_nn_64x16x64x16x16
 14.45%  39.387ms       256  153.85us  151.21us  160.87us  sgemm_sm35_ldg_nt_64x16x64x16x16
  8.82%  24.052ms       256  93.952us  92.485us  95.621us  void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=0, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)
  3.89%  10.617ms       512  20.736us  20.257us  21.378us  void caffe::im2col_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, caffe::im2col_gpu_kernel<float>*)
  3.34%  9.1044ms       263  34.617us  3.7120us  3.8673ms  [CUDA memset]
  2.79%  7.5948ms       256  29.667us  28.962us  31.842us  void caffe::col2im_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, caffe::col2im_gpu_kernel<float>*)
  0.55%  1.5069ms       256  5.8860us  5.7920us  6.0170us  void scal_kernel<float, int=1, bool=0, int=6, int=5, int=5, int=3>(cublasTransposeParams<float>, float const *, float*, float const *)
  0.01%  24.737us         5  4.9470us  1.3760us  9.0240us  [CUDA memcpy HtoD]

==22152== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 59.07%  421.16ms        12  35.097ms  9.7840us  404.85ms  cudaFree
 16.58%  118.21ms         5  23.642ms  24.311us  114.40ms  cudaMemcpy
 16.09%  114.75ms         6  19.125ms  4.4150us  84.391ms  cudaEventSynchronize
  3.79%  27.056ms      2304  11.742us  8.5900us  164.51us  cudaLaunch
  2.31%  16.464ms        17  968.50us  9.6240us  15.122ms  cudaMallocHost
  0.75%  5.3467ms     26368     202ns     163ns  309.15us  cudaSetupArgument
  0.40%  2.8448ms       256  11.112us  9.3900us  24.248us  cudaMemsetAsync
  0.30%  2.1045ms        13  161.88us  11.693us  577.87us  cudaMalloc
  0.14%  963.59us        17  56.681us  11.621us  258.00us  cudaFreeHost
  0.10%  716.98us      2304     311ns     194ns  17.913us  cudaConfigureCall
  0.10%  685.03us       268  2.5560us  1.2250us  167.08us  cudaEventRecord
  0.09%  629.18us       249  2.5260us     139ns  105.21us  cuDeviceGetAttribute
  0.08%  590.73us      2048     288ns     154ns  1.7830us  cudaGetLastError
  0.07%  528.10us       256  2.0620us  1.7080us  5.4530us  cudaEventQuery
  0.03%  240.69us       768     313ns     257ns  1.6600us  cudaPeekAtLastError
  0.03%  216.16us         1  216.16us  216.16us  216.16us  cudaGetDeviceProperties
  0.02%  148.53us         7  21.218us  10.140us  38.556us  cudaMemset
  0.01%  84.695us         3  28.231us  26.699us  31.207us  cuDeviceTotalMem
  0.01%  78.621us         3  26.207us  21.760us  31.769us  cuDeviceGetName
  0.01%  46.214us        23  2.0090us     470ns  20.890us  cudaGetDevice
  0.00%  21.032us         7  3.0040us  2.0100us  4.6780us  cudaEventElapsedTime
  0.00%  17.786us        10  1.7780us     844ns  5.8780us  cudaEventCreate
  0.00%  17.266us        20     863ns     391ns  2.0010us  cudaSetDevice
  0.00%  14.186us        16     886ns     603ns  2.7640us  cudaEventCreateWithFlags
  0.00%  12.532us        10  1.2530us     826ns  3.3460us  cudaEventDestroy
  0.00%  7.2620us        10     726ns     320ns  3.0300us  cudaDeviceGetAttribute
  0.00%  2.7700us         4     692ns     273ns  1.5150us  cuDeviceGetCount
  0.00%  1.7120us         2     856ns     716ns     996ns  cuInit
  0.00%  1.4990us         4     374ns     204ns     646ns  cuDeviceGet
  0.00%  1.2270us         2     613ns     522ns     705ns  cuDriverGetVersion
I0113 13:34:55.833313 22162 caffe.cpp:297] Use GPU with device ID 0
==22162== NVPROF is profiling process 22162, command: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv3.prototxt --iterations=1 --gpu 0 --logtostderr=1
I0113 13:34:59.687428 22162 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: proto_forceGradInput/conv3.prototxt
I0113 13:34:59.687537 22162 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0113 13:34:59.687664 22162 net.cpp:50] Initializing net from parameters: 
name: "ConvLayer_128x128x9x9"
input: "data"
input_dim: 128
input_dim: 128
input_dim: 32
input_dim: 32
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "data"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 9
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
I0113 13:34:59.687717 22162 net.cpp:435] Input 0 -> data
I0113 13:34:59.703012 22162 layer_factory.hpp:76] Creating layer conv3
I0113 13:34:59.703052 22162 net.cpp:110] Creating Layer conv3
I0113 13:34:59.703065 22162 net.cpp:477] conv3 <- data
I0113 13:34:59.703083 22162 net.cpp:433] conv3 -> conv3
I0113 13:34:59.717646 22162 net.cpp:155] Setting up conv3
I0113 13:34:59.717689 22162 net.cpp:163] Top shape: 128 128 24 24 (9437184)
I0113 13:34:59.717720 22162 net.cpp:240] conv3 does not need backward computation.
I0113 13:34:59.717730 22162 net.cpp:283] This network produces output conv3
I0113 13:34:59.717743 22162 net.cpp:297] Network initialization done.
I0113 13:34:59.717751 22162 net.cpp:298] Memory required for data: 37748736
I0113 13:34:59.717777 22162 caffe.cpp:309] Performing Forward
I0113 13:34:59.727598 22162 caffe.cpp:314] Initial loss: 0
I0113 13:34:59.727637 22162 caffe.cpp:315] Performing Backward
I0113 13:34:59.794772 22162 caffe.cpp:323] *** Benchmark begins ***
I0113 13:34:59.794791 22162 caffe.cpp:324] Testing for 1 iterations.
I0113 13:35:00.828299 22162 caffe.cpp:352] Iteration: 1 forward-backward time: 554.04 ms.
I0113 13:35:00.828375 22162 caffe.cpp:355] Average time per layer: 
I0113 13:35:00.828385 22162 caffe.cpp:358]      conv3	forward: 218.581 ms.
I0113 13:35:00.828397 22162 caffe.cpp:361]      conv3	backward: 335.339 ms.
I0113 13:35:00.828420 22162 caffe.cpp:366] Average Forward pass: 218.603 ms.
I0113 13:35:00.828433 22162 caffe.cpp:368] Average Backward pass: 335.41 ms.
I0113 13:35:00.828450 22162 caffe.cpp:370] Average Forward-Backward: 554.185 ms.
I0113 13:35:00.828469 22162 caffe.cpp:372] Total Time: 554.185 ms.
I0113 13:35:00.828480 22162 caffe.cpp:373] *** Benchmark ends ***
==22162== Profiling application: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv3.prototxt --iterations=1 --gpu 0 --logtostderr=1
==22162== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 35.03%  390.45ms       256  1.5252ms  1.5181ms  1.6605ms  sgemm_sm35_ldg_nn_64x16x64x16x16
 26.30%  293.10ms       256  1.1449ms  1.1399ms  1.1557ms  sgemm_sm35_ldg_tn_32x16x64x8x16
 18.65%  207.86ms       256  811.96us  804.49us  819.05us  sgemm_sm_heavy_nt_ldg
  8.46%  94.294ms       256  368.33us  362.87us  374.42us  void caffe::col2im_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, caffe::col2im_gpu_kernel<float>*)
  7.63%  85.005ms       512  166.03us  161.16us  171.31us  void caffe::im2col_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, caffe::im2col_gpu_kernel<float>*)
  3.24%  36.120ms       256  141.09us  128.17us  156.74us  sgemm_sm35_ldg_nt_64x16x128x8x32
  0.38%  4.2709ms       256  16.683us  16.256us  17.345us  void gemv2T_kernel_val<float, int=128, int=16, int=2, int=2, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)
  0.15%  1.7218ms       256  6.7250us  6.3360us  7.3280us  void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=0, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)
  0.11%  1.1983ms         7  171.19us  4.3520us  335.47us  [CUDA memset]
  0.05%  529.05us         5  105.81us  1.2800us  519.04us  [CUDA memcpy HtoD]

==22162== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 34.27%  531.03ms         6  88.506ms  3.6640us  321.64ms  cudaEventSynchronize
 31.10%  481.97ms         5  96.393ms  23.198us  479.48ms  cudaMemcpy
 27.48%  425.86ms        12  35.488ms  10.561us  409.34ms  cudaFree
  5.03%  77.995ms      2304  33.851us  8.5950us  1.5327ms  cudaLaunch
  1.18%  18.329ms        17  1.0782ms  9.3540us  15.208ms  cudaMallocHost
  0.46%  7.1354ms     28160     253ns     164ns  973.59us  cudaSetupArgument
  0.10%  1.6007ms        17  94.160us  11.692us  833.34us  cudaFreeHost
  0.10%  1.5196ms        13  116.89us  10.441us  195.52us  cudaMalloc
  0.05%  788.49us      2304     342ns     185ns  8.5040us  cudaConfigureCall
  0.04%  668.37us       524  1.2750us     921ns  41.388us  cudaEventRecord
  0.04%  607.40us       249  2.4390us     139ns  94.190us  cuDeviceGetAttribute
  0.03%  466.74us       512     911ns     779ns  8.4330us  cudaStreamWaitEvent
  0.03%  420.55us      1280     328ns     257ns  12.356us  cudaGetLastError
  0.02%  286.83us       768     373ns     280ns  11.068us  cudaPeekAtLastError
  0.02%  279.74us       256  1.0920us     929ns  6.7340us  cudaStreamGetPriority
  0.01%  188.57us         1  188.57us  188.57us  188.57us  cudaGetDeviceProperties
  0.01%  123.66us         7  17.665us  10.246us  27.682us  cudaMemset
  0.01%  81.853us         3  27.284us  24.503us  30.820us  cuDeviceGetName
  0.01%  81.075us         3  27.025us  26.360us  27.670us  cuDeviceTotalMem
  0.00%  60.944us        23  2.6490us     483ns  27.480us  cudaGetDevice
  0.00%  55.907us         2  27.953us  21.247us  34.660us  cudaStreamCreateWithFlags
  0.00%  31.836us         7  4.5480us  2.7860us  7.7620us  cudaEventElapsedTime
  0.00%  18.410us        20     920ns     393ns  2.4080us  cudaSetDevice
  0.00%  18.000us        18  1.0000us     611ns  3.0870us  cudaEventCreateWithFlags
  0.00%  16.930us        10  1.6930us     800ns  5.0050us  cudaEventCreate
  0.00%  13.993us        10  1.3990us     794ns  4.9980us  cudaEventDestroy
  0.00%  6.8740us        10     687ns     321ns  2.9370us  cudaDeviceGetAttribute
  0.00%  2.9360us         4     734ns     305ns  1.4130us  cuDeviceGetCount
  0.00%  1.8420us         4     460ns     267ns     825ns  cuDeviceGet
  0.00%  1.8100us         2     905ns     736ns  1.0740us  cuInit
  0.00%  1.2050us         2     602ns     443ns     762ns  cuDriverGetVersion
I0113 13:35:01.487551 22173 caffe.cpp:297] Use GPU with device ID 0
==22173== NVPROF is profiling process 22173, command: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv4.prototxt --iterations=1 --gpu 0 --logtostderr=1
I0113 13:35:05.381098 22173 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: proto_forceGradInput/conv4.prototxt
I0113 13:35:05.381214 22173 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0113 13:35:05.381361 22173 net.cpp:50] Initializing net from parameters: 
name: "ConvLayer_128x128x7x7"
input: "data"
input_dim: 128
input_dim: 128
input_dim: 16
input_dim: 16
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "data"
  top: "conv4"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 128
    kernel_size: 7
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
I0113 13:35:05.381419 22173 net.cpp:435] Input 0 -> data
I0113 13:35:05.399667 22173 layer_factory.hpp:76] Creating layer conv4
I0113 13:35:05.399714 22173 net.cpp:110] Creating Layer conv4
I0113 13:35:05.399730 22173 net.cpp:477] conv4 <- data
I0113 13:35:05.399749 22173 net.cpp:433] conv4 -> conv4
I0113 13:35:05.410348 22173 net.cpp:155] Setting up conv4
I0113 13:35:05.410392 22173 net.cpp:163] Top shape: 128 128 10 10 (1638400)
I0113 13:35:05.410424 22173 net.cpp:240] conv4 does not need backward computation.
I0113 13:35:05.410436 22173 net.cpp:283] This network produces output conv4
I0113 13:35:05.410452 22173 net.cpp:297] Network initialization done.
I0113 13:35:05.410461 22173 net.cpp:298] Memory required for data: 6553600
I0113 13:35:05.410501 22173 caffe.cpp:309] Performing Forward
I0113 13:35:05.425209 22173 caffe.cpp:314] Initial loss: 0
I0113 13:35:05.425251 22173 caffe.cpp:315] Performing Backward
I0113 13:35:05.439846 22173 caffe.cpp:323] *** Benchmark begins ***
I0113 13:35:05.439869 22173 caffe.cpp:324] Testing for 1 iterations.
I0113 13:35:05.622223 22173 caffe.cpp:352] Iteration: 1 forward-backward time: 105.715 ms.
I0113 13:35:05.622270 22173 caffe.cpp:355] Average time per layer: 
I0113 13:35:05.622282 22173 caffe.cpp:358]      conv4	forward: 44.6497 ms.
I0113 13:35:05.622297 22173 caffe.cpp:361]      conv4	backward: 61.0004 ms.
I0113 13:35:05.622323 22173 caffe.cpp:366] Average Forward pass: 44.6712 ms.
I0113 13:35:05.622339 22173 caffe.cpp:368] Average Backward pass: 61.0179 ms.
I0113 13:35:05.622359 22173 caffe.cpp:370] Average Forward-Backward: 105.83 ms.
I0113 13:35:05.622378 22173 caffe.cpp:372] Total Time: 105.83 ms.
I0113 13:35:05.622391 22173 caffe.cpp:373] *** Benchmark ends ***
==22173== Profiling application: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv4.prototxt --iterations=1 --gpu 0 --logtostderr=1
==22173== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 36.63%  73.510ms       256  287.15us  276.43us  303.70us  void sgemm_largek_lds64<bool=0, bool=0, int=5, int=5, int=4, int=4, int=4, int=32>(float*, float const *, float const *, int, int, int, int, int, int, float const *, float const *, float, float, int, int, int*, int*)
 24.39%  48.949ms       256  191.21us  189.77us  193.68us  sgemm_sm35_ldg_tn_32x16x64x8x16
 19.71%  39.552ms       256  154.50us  151.05us  159.27us  sgemm_sm35_ldg_nt_64x16x64x16x16
  8.51%  17.085ms       512  33.368us  32.545us  34.306us  void caffe::im2col_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, caffe::im2col_gpu_kernel<float>*)
  8.26%  16.579ms       256  64.763us  63.971us  66.179us  void caffe::col2im_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, caffe::col2im_gpu_kernel<float>*)
  0.71%  1.4186ms       256  5.5410us  5.4080us  6.0800us  void gemv2T_kernel_val<float, int=128, int=16, int=2, int=2, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)
  0.69%  1.3902ms       263  5.2850us  4.3200us  85.253us  [CUDA memset]
  0.62%  1.2431ms       256  4.8550us  4.5760us  5.6960us  void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=0, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)
  0.32%  648.78us       256  2.5340us  2.1440us  2.7520us  void scal_kernel<float, int=1, bool=1, int=6, int=5, int=5, int=3>(cublasTransposeParams<float>, float const *, float*, float const *)
  0.16%  322.93us         5  64.586us  1.0240us  314.16us  [CUDA memcpy HtoD]

==22173== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 68.11%  479.84ms        12  39.987ms  10.691us  460.46ms  cudaFree
 11.22%  79.046ms         6  13.174ms  4.4630us  48.506ms  cudaEventSynchronize
 11.00%  77.498ms         5  15.500ms  19.026us  76.818ms  cudaMemcpy
  4.27%  30.110ms      2304  13.068us  10.398us  1.0987ms  cudaLaunch
  2.85%  20.093ms        17  1.1819ms  9.9770us  18.139ms  cudaMallocHost
  1.01%  7.1437ms     26368     270ns     204ns  18.192us  cudaSetupArgument
  0.49%  3.4754ms       256  13.575us  11.498us  30.985us  cudaMemsetAsync
  0.22%  1.5430ms        13  118.69us  15.200us  196.52us  cudaMalloc
  0.17%  1.1651ms        17  68.537us  14.158us  629.09us  cudaFreeHost
  0.13%  931.54us      2304     404ns     236ns  10.078us  cudaConfigureCall
  0.11%  796.12us      2048     388ns     194ns  11.665us  cudaGetLastError
  0.09%  665.27us       256  2.5980us  2.1670us  22.065us  cudaEventQuery
  0.09%  642.42us       249  2.5790us     141ns  106.13us  cuDeviceGetAttribute
  0.07%  480.28us       268  1.7920us  1.4590us  13.787us  cudaEventRecord
  0.05%  334.41us       768     435ns     320ns  8.2970us  cudaPeekAtLastError
  0.03%  212.90us         1  212.90us  212.90us  212.90us  cudaGetDeviceProperties
  0.02%  161.82us         7  23.117us  20.040us  32.929us  cudaMemset
  0.01%  86.199us         3  28.733us  26.581us  31.666us  cuDeviceTotalMem
  0.01%  86.152us         3  28.717us  23.930us  36.680us  cuDeviceGetName
  0.01%  64.778us        23  2.8160us     468ns  22.718us  cudaGetDevice
  0.00%  24.492us         7  3.4980us  2.5510us  4.9540us  cudaEventElapsedTime
  0.00%  20.030us        20  1.0010us     466ns  1.7220us  cudaSetDevice
  0.00%  17.648us        16  1.1030us     756ns  3.4890us  cudaEventCreateWithFlags
  0.00%  17.536us        10  1.7530us  1.0110us  6.9910us  cudaEventCreate
  0.00%  15.582us        10  1.5580us     964ns  4.6000us  cudaEventDestroy
  0.00%  8.1520us        10     815ns     400ns  3.3550us  cudaDeviceGetAttribute
  0.00%  3.1170us         4     779ns     302ns  1.5270us  cuDeviceGetCount
  0.00%  1.9640us         2     982ns     915ns  1.0490us  cuInit
  0.00%  1.6620us         4     415ns     232ns     747ns  cuDeviceGet
  0.00%  1.2610us         2     630ns     446ns     815ns  cuDriverGetVersion
I0113 13:35:06.348927 22183 caffe.cpp:297] Use GPU with device ID 0
==22183== NVPROF is profiling process 22183, command: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv5.prototxt --iterations=1 --gpu 0 --logtostderr=1
I0113 13:35:09.211032 22183 upgrade_proto.cpp:618] Attempting to upgrade input file specified using deprecated V1LayerParameter: proto_forceGradInput/conv5.prototxt
I0113 13:35:09.211133 22183 upgrade_proto.cpp:626] Successfully upgraded file specified using deprecated V1LayerParameter
I0113 13:35:09.211266 22183 net.cpp:50] Initializing net from parameters: 
name: "ConvLayer_384x384x3x3"
input: "data"
input_dim: 128
input_dim: 384
input_dim: 13
input_dim: 13
force_backward: true
state {
  phase: TRAIN
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "data"
  top: "conv5"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 384
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
I0113 13:35:09.211323 22183 net.cpp:435] Input 0 -> data
I0113 13:35:09.229449 22183 layer_factory.hpp:76] Creating layer conv5
I0113 13:35:09.229496 22183 net.cpp:110] Creating Layer conv5
I0113 13:35:09.229511 22183 net.cpp:477] conv5 <- data
I0113 13:35:09.229538 22183 net.cpp:433] conv5 -> conv5
I0113 13:35:09.247714 22183 net.cpp:155] Setting up conv5
I0113 13:35:09.247763 22183 net.cpp:163] Top shape: 128 384 11 11 (5947392)
I0113 13:35:09.247795 22183 net.cpp:240] conv5 does not need backward computation.
I0113 13:35:09.247808 22183 net.cpp:283] This network produces output conv5
I0113 13:35:09.247825 22183 net.cpp:297] Network initialization done.
I0113 13:35:09.247835 22183 net.cpp:298] Memory required for data: 23789568
I0113 13:35:09.247864 22183 caffe.cpp:309] Performing Forward
I0113 13:35:09.257097 22183 caffe.cpp:314] Initial loss: 0
I0113 13:35:09.257139 22183 caffe.cpp:315] Performing Backward
I0113 13:35:09.274727 22183 caffe.cpp:323] *** Benchmark begins ***
I0113 13:35:09.274750 22183 caffe.cpp:324] Testing for 1 iterations.
I0113 13:35:09.513859 22183 caffe.cpp:352] Iteration: 1 forward-backward time: 132.481 ms.
I0113 13:35:09.513906 22183 caffe.cpp:355] Average time per layer: 
I0113 13:35:09.513917 22183 caffe.cpp:358]      conv5	forward: 50.0622 ms.
I0113 13:35:09.513931 22183 caffe.cpp:361]      conv5	backward: 82.348 ms.
I0113 13:35:09.513957 22183 caffe.cpp:366] Average Forward pass: 50.0828 ms.
I0113 13:35:09.513973 22183 caffe.cpp:368] Average Backward pass: 82.3706 ms.
I0113 13:35:09.513993 22183 caffe.cpp:370] Average Forward-Backward: 132.599 ms.
I0113 13:35:09.514014 22183 caffe.cpp:372] Total Time: 132.599 ms.
I0113 13:35:09.514026 22183 caffe.cpp:373] *** Benchmark ends ***
==22183== Profiling application: ./caffe/build/tools/caffe time --model=proto_forceGradInput/conv5.prototxt --iterations=1 --gpu 0 --logtostderr=1
==22183== Profiling result:
Time(%)      Time     Calls       Avg       Min       Max  Name
 33.94%  91.011ms       256  355.51us  354.90us  356.92us  sgemm_sm35_ldg_nn_64x16x64x16x16
 21.67%  58.113ms       256  227.00us  223.18us  234.83us  sgemm_sm35_ldg_nt_64x16x64x16x16
 20.18%  54.107ms       256  211.35us  206.92us  249.52us  sgemm_sm35_ldg_tn_128x8x256x16x32
 15.03%  40.291ms       256  157.39us  117.38us  212.17us  sgemm_sm35_ldg_tn_64x16x128x8x32
  3.99%  10.704ms       512  20.906us  19.585us  22.273us  void caffe::im2col_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, caffe::im2col_gpu_kernel<float>*)
  3.55%  9.5250ms       256  37.207us  36.578us  38.178us  void caffe::col2im_gpu_kernel<float>(int, float const *, int, int, int, int, int, int, int, int, int, int, int, caffe::col2im_gpu_kernel<float>*)
  0.67%  1.8076ms       256  7.0610us  6.7520us  7.8720us  void gemv2T_kernel_val<float, int=128, int=16, int=2, int=2, bool=0>(int, int, float, float const *, int, float const *, int, float, float*, int)
  0.54%  1.4538ms       256  5.6790us  5.4400us  5.9520us  void gemmk1_kernel<float, int=256, int=5, bool=0, bool=0, bool=0, bool=0>(cublasGemmk1Params<float>, float const *, float const *, float*)
  0.23%  620.61us         7  88.658us  4.3520us  168.78us  [CUDA memset]
  0.20%  524.64us         5  104.93us  1.2480us  515.61us  [CUDA memcpy HtoD]

==22183== API calls:
Time(%)      Time     Calls       Avg       Min       Max  Name
 62.98%  477.93ms        12  39.827ms  10.729us  458.45ms  cudaFree
 14.19%  107.70ms         5  21.541ms  19.919us  106.89ms  cudaMemcpy
 14.13%  107.26ms         6  17.876ms  4.2840us  65.690ms  cudaEventSynchronize
  3.86%  29.273ms      2304  12.705us  10.334us  49.282us  cudaLaunch
  2.85%  21.652ms        17  1.2737ms  11.648us  18.029ms  cudaMallocHost
  0.91%  6.9045ms     28160     245ns     203ns  327.21us  cudaSetupArgument
  0.24%  1.7896ms        17  105.27us  13.819us  971.12us  cudaFreeHost
  0.22%  1.7006ms        13  130.82us  14.644us  203.86us  cudaMalloc
  0.12%  885.91us      2304     384ns     242ns  7.2230us  cudaConfigureCall
  0.09%  717.83us       524  1.3690us  1.1220us  6.4170us  cudaEventRecord
  0.08%  623.79us       249  2.5050us     140ns  103.78us  cuDeviceGetAttribute
  0.07%  548.87us       512  1.0720us     954ns  2.7770us  cudaStreamWaitEvent
  0.06%  478.30us      1280     373ns     324ns  2.0680us  cudaGetLastError
  0.04%  340.52us       256  1.3300us  1.1940us  4.0000us  cudaStreamGetPriority
  0.04%  303.86us       768     395ns     323ns  1.9640us  cudaPeekAtLastError
  0.03%  212.53us         1  212.53us  212.53us  212.53us  cudaGetDeviceProperties
  0.02%  158.57us         7  22.653us  12.950us  34.584us  cudaMemset
  0.01%  83.316us         3  27.772us  26.227us  30.826us  cuDeviceTotalMem
  0.01%  80.844us         3  26.948us  21.030us  35.411us  cuDeviceGetName
  0.01%  67.220us         2  33.610us  31.497us  35.723us  cudaStreamCreateWithFlags
  0.01%  52.537us        23  2.2840us     508ns  21.547us  cudaGetDevice
  0.00%  30.549us         7  4.3640us  3.2320us  5.6950us  cudaEventElapsedTime
  0.00%  21.758us        20  1.0870us     493ns  1.8420us  cudaSetDevice
  0.00%  21.315us        18  1.1840us     752ns  3.4750us  cudaEventCreateWithFlags
  0.00%  20.272us        10  2.0270us  1.0250us  6.0360us  cudaEventCreate
  0.00%  14.891us        10  1.4890us     964ns  4.0650us  cudaEventDestroy
  0.00%  7.5090us        10     750ns     394ns  2.7050us  cudaDeviceGetAttribute
  0.00%  3.1130us         4     778ns     288ns  1.6410us  cuDeviceGetCount
  0.00%  2.2070us         2  1.1030us  1.0570us  1.1500us  cuInit
  0.00%  1.3080us         4     327ns     180ns     536ns  cuDeviceGet
  0.00%  1.1660us         2     583ns     410ns     756ns  cuDriverGetVersion
